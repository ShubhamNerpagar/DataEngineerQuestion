{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27f112bf-f1a0-4502-9bbe-ca1011a3f476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# DataFrame Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1874fcc6-f428-4ea6-b192-78202aaff370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrame_Transfrom\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3c5063-bbcd-4e51-8c03-98a812fb2335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-----------+-----------+\n| id|    name|age|salary|    address|    nominee|\n+---+--------+---+------+-----------+-----------+\n|  1|  Manish| 26| 75000|      Bihar|   nominee1|\n|  2|  Nikita| 23|100000|Maharashtra|   nominee2|\n|  3|  Pritam| 22|150000|   banglore|      India|\n|  4|Prantosh| 17|200000|    kolkata|      india|\n|  5|  Vikash| 31|300000|       pune|Maharashtra|\n|  6|   Rahul| 55|300000|       null|       null|\n+---+--------+---+------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.format(\"csv\")\\\n",
    "                        .option(\"header\",\"true\")\\\n",
    "                        .option(\"inferschema\",\"true\")\\\n",
    "                        .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                        .load(\"/FileStore/schnario/emp_data3.csv\")\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a957f6-091a-444d-aa58-14878107fb27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType, StringType, StructField, IntegerType\n",
    "\n",
    "\n",
    "emp_data = StructType([\n",
    "                        StructField(\"id\", IntegerType(), True),\\\n",
    "                        StructField(\"name\", StringType(), True),\\\n",
    "                        StructField(\"age\", IntegerType(), True),\\\n",
    "                        StructField(\"salary\", IntegerType(), True),\\\n",
    "                        StructField(\"address\", StringType(), True),\\\n",
    "                        StructField(\"nominee\", StringType(), True),\\\n",
    "                        StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9702c236-7316-477b-a51c-0de12ad4f723",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Spark transformation\n",
    "\n",
    "employee_df.select('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a243ea67-1407-4502-b0d7-06c03dce898e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employee_df.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfb26a8-c0c2-427b-81bf-137bd132494c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "employee_df.select(expr(\"id + 5\").alias(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0107258-ecbc-42db-ab8b-23c7b7d7ed5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|   Name+salary|\n+--------------+\n|   Manish75000|\n|  Nikita100000|\n|  Pritam150000|\n|Prantosh200000|\n|  Vikash300000|\n|   Rahul300000|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import concat\n",
    "\n",
    "employee_df.select(expr(\"concat(name,salary)\").alias(\"Name+salary\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Question_4_dataframe_transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
